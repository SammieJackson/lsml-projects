# Factorization Machine

Необходимо сделать приятную в использовании и быструю реализацию [Factorization Machine](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)

### Интерфейс

Утилита должна иметь два режима - обучение и применение. Работать нужно со стандартными файлами csv. При обучении она должна принимать, как минимум, путь к файлу с данными, тип обучаемого алгоритма, максимальный допустимый размер модели и темп обучения. Модель нужно уметь сохранять на диск и применять к тестовому файлу.

### Принцип работы

Хочется, чтобы утилита была быстра и качественна. Постарайтесь применить все описанные на лекциях и семинарах трюки, хешируйте фичи, следите за эффективностью работы с памятью. 

На датасетах описанных ниже постарайтесь победить vowpall wabbit по качеству.

### Процесс сдачи задания:

0) Проект расчитан на 1-2 человек.

1) Реализуйте бибилиотеку, так как написанно выше. Сделать это надо на любом *компилируемом* языке, позаботившись о качестве кода. 

2) Библиотеку разместите на github.  Напишите к ней приятный ридми, в котором будет общее описание библиотеки и инструкция о том как её запустить и использовать.

3)  Оцените скорость работы и качество полученной модели на датасетах [MovieLens ](https://grouplens.org/datasets/movielens/) (ML-100k для быстрых тестов и ML-20M для финального отчета) и [ avazu](https://www.kaggle.com/c/avazu-ctr-predictio).  О результатах бенчмарков так же напишите в ридми вашего репозитория.

4) Мы почитаем ваш репозиторий и поставим вам оценку. Оцениваться будет качество результата, удобство и простота использования, скорость, а так же качество кода.

**Bonus**
Попробуйте ускорить обучение с помощью Hogwild! (см. 3-ю лекцию) или 

Recht, B., Re, C., Wright, S., & Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems (pp. 693-701).

Такая техника используется в программе libFFM (модификация факторизационных машин)

Juan, Y., Zhuang, Y., Chin, W. S., & Lin, C. J. (2016, September). Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). ACM.

Интересно попробовать другие способы ускорения стохастического градиента, например SVRG. (см 2-ю лекцию).
