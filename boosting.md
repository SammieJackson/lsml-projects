# Gradient Boosting 

Вам необходимо реализовать утилиту, позволяющую обучать градиентный бустинг на решающих деревьях быстро и качественно, сохранять модель на диск, а так же применять обученные модельки. 

### Интерфейс

Утилита должна иметь два режима - обучение и применение. Работать она должна со стандартными файлами csv. При обучении она должна принимать, как минимум, путь к файлу с данными, количество деревьев и learning rate в качестве параметров и возвращать обученную модель.  В процессе обучения она должна логировать лосс после обучения каждого дерева. В режиме приминения она должна применять ранее обученную модель к тестовым данным и возвращать файл с предсказаниями. При обучении модели нужно использовать все ядра процессора (если хотите, то можно реализовать её на видеокарте).

### Принцип работы

Хочется сделать акцент на скорость работы, и быть, по крайней мере, не медленнее чем библиотека xgboost (а лучше lightgbm). Для этого, проще всего бустинг реализовывать на oblivious decision trees (если не знаете что это - в [этой](https://arxiv.org/pdf/1609.05610.pdf) статье есть рассказ о них). Бустинг должен поддерживать режимы классификации и регресии те уметь минимизировать, как минимум MSE и LogLoss. 

Внутри предлагается реализовать популярный [histogram-based подход](https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst).

### Процесс сдачи задания:

1) Реализуйте бибилиотеку, так как написанно выше. Сделать это надо на любом *компилируемом* языке, позаботившись о качестве кода. 

2) Библиотеку разместите на github.  Напишите к ней приятный ридми, в котором будет общее описание библиотеки и инструкция о том как её запустить и использовать.

3) Сравните свою библиотеку с популярными аналогами — *xgboost*, *lightgbm* и catboost. Оцените скорость работы и качество полученной модели на открытых датасетах.

4) Выполняйте правила промежуточных отчетов, описанные на странице курса.

